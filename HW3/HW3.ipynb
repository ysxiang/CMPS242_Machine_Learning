{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/matplotlib/__init__.py:962: UserWarning: Duplicate key in file \"/Users/xiangyusi/.matplotlib/matplotlibrc\", line #2\n",
      "  (fname, cnt))\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "import sys\n",
    "reload(sys)\n",
    "sys.setdefaultencoding('ISO-8859-1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Load the training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_column', None)\n",
    "training_set = pd.read_csv('train.csv')\n",
    "\n",
    "# we want to separate the txt messages into two groups: ham(1) and spam(0)\n",
    "ham_or_spam = training_set['label']\n",
    "training_set_target = [] # same as the label in Ziyi's version\n",
    "for i in range(len(ham_or_spam)):\n",
    "    if ham_or_spam[i] == \"ham\":\n",
    "        training_set_target.append(1)\n",
    "    else:\n",
    "        training_set_target.append(0)\n",
    "# y is the array version\n",
    "y = training_set_target\n",
    "training_set_target = np.transpose(np.matrix(training_set_target))\n",
    "training_set_content = training_set['sms'].as_matrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'i',\n",
       " u'me',\n",
       " u'my',\n",
       " u'myself',\n",
       " u'we',\n",
       " u'our',\n",
       " u'ours',\n",
       " u'ourselves',\n",
       " u'you',\n",
       " u'your']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "''' Load the stopwords '''\n",
    "stop_words = stopwords.words('english')\n",
    "stop_words[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Tokenizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hey pay salary de lt gt',\n",
       " 'happen dear silent tensed',\n",
       " 'want please inside outside bedroom',\n",
       " 'wonder got online love gone net cafe get phone recharged friends net think boytoy',\n",
       " 'dad gon na call gets work ask crazy questions',\n",
       " 'good afternoon love goes day hope maybe got leads job think boytoy send passionate kiss across sea',\n",
       " \"u repeat e instructions wat 's e road name ur house\",\n",
       " 'yo dude guess got arrested day',\n",
       " 'nokia tone ur mob every week txt nok 1st tone free get txtin tell ur friends 150p/tone reply hl 4info',\n",
       " 'hello damn christmas thing think decided keep mp3 doesnt work']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def tokenization(text):\n",
    "    tokens=[]\n",
    "    for word in nltk.word_tokenize(text):\n",
    "        if re.search('[a-zA-Z]', word) and word.lower() not in stop_words:\n",
    "            tokens.append(word.lower())\n",
    "    return tokens\n",
    "\n",
    "training_set_corpus_tokenized = []\n",
    "for i in training_set_content:\n",
    "    training_set_corpus_tokenized.append(' '.join(tokenization(i)))\n",
    "    \n",
    "training_set_corpus_tokenized[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Applying TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(decode_error = 'ignore')\n",
    "transformer = TfidfTransformer(norm = 'l2', use_idf = True)\n",
    "tfidf_matrix = transformer.fit_transform(vectorizer.fit_transform(training_set_corpus_tokenized))\n",
    "\n",
    "X = tfidf_matrix.toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Define functions for logistic regression\n",
    "Because do not want to regularize the bias term (in order to shift the plot) ==> separate the gradient of the first dimension apart from the others."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_bias_term(X):\n",
    "    X = np.insert(X, [0], [1], axis=1)\n",
    "    return X\n",
    "\n",
    "def sigmoid(z):\n",
    "    sigma = 1 / (1 + np.exp(-z))\n",
    "    return sigma\n",
    "\n",
    "def loss_function(X, y, w, lmd):\n",
    "    # X is the training set, 3000 x 3000, w is the weight vector, 3000 x 1\n",
    "    # y is the target, 3000 x 1, lmd is the hyper parameter for regularization\n",
    "    m = np.shape(y)[0]\n",
    "    y = np.transpose(y)\n",
    "    hypo = sigmoid(np.dot(X, w))\n",
    "    # avoid the case of log(0)\n",
    "    for i in range(len(hypo)):\n",
    "        if hypo[i,0] == 1:\n",
    "            hypo[i,0] = 0.99999\n",
    "        elif hypo[i,0] < 0.00001:\n",
    "            hypo[i,0] = 0.00001\n",
    "    loss = (1.0 / m) * (-np.dot(y, np.log(hypo)) - np.dot((1 - y), np.log(1 - hypo))) + lmd * (1.0 / m) * np.dot(np.transpose(w), w)\n",
    "    return loss\n",
    "\n",
    "def gradient_descent(X, y, w_, eta, lmd):\n",
    "    m = np.shape(y)[0]\n",
    "    hypo = sigmoid(np.dot(X, w_))\n",
    "    loss_history = []\n",
    "    # store every previous loss value to monitor whether the gradient descent is improving\n",
    "    # this mechanism also helps to avoid 'overshooting'\n",
    "    prev_loss = 9999.0\n",
    "    for i in range(1, 1001):\n",
    "        # below is the general form of gradient descent:\n",
    "        # w_ = w_ - (1.0 / m) * eta * (i ** (-0.9)) * np.dot(np.transpose(X), hypo - y) + lmd * w_\n",
    "        w_[0,:] = w_[0,:] - (1.0 / m) * eta * (i ** (-0.9)) * np.dot(np.transpose(X)[0,:], hypo - y)\n",
    "        w_[1:,:] = w_[1:,:] - (1.0 / m) * eta * (i ** (-0.9)) * np.dot(np.transpose(X)[1:,:], hypo - y) + lmd * 1.0 / m * w_[1:,:]\n",
    "        crt_loss = loss_function(X, y, w_, lmd)[0,0]\n",
    "        if crt_loss > prev_loss:\n",
    "            break\n",
    "        loss_history.append(crt_loss)\n",
    "        prev_loss = crt_loss\n",
    "    print loss_history\n",
    "    return w_\n",
    "\n",
    "def predict(X, y, w):\n",
    "    predict_hypo = sigmoid(np.dot(X, w))\n",
    "    p = predict_hypo\n",
    "    for i in range(np.shape(y)[0]):\n",
    "        if predict_hypo[i,0] >= 0.5:\n",
    "            p[i,0] = 1\n",
    "        else:\n",
    "            p[i,0] = 0\n",
    "    return p\n",
    "\n",
    "def compute_accuracy(X, y, w):\n",
    "    result = predict(X, y, w)\n",
    "    wrong_answer = 0.0\n",
    "    for i in range(np.shape(y)[0]):\n",
    "        if result[i,0] != y[i,0]:\n",
    "            wrong_answer = wrong_answer + 1\n",
    "    accuracy = 1 - wrong_answer / np.shape(y)[0]\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the model. <br>\n",
    "Originally set the learning rate to be 1, and it continues to drop down. Also, because don't have bias term, set regrlarizer to 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[-0.00055179],\n",
       "        [-0.00048387],\n",
       "        [-0.00048387],\n",
       "        ...,\n",
       "        [ 0.00066489],\n",
       "        [ 0.00050107],\n",
       "        [ 0.00028816]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w = np.transpose(np.matrix(np.zeros(np.shape(X)[1])))\n",
    "w = gradient_descent(X, training_set_target, w, 1, 0)\n",
    "\n",
    "w"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute the accuracy of training set to evaluate the model and get the accuracy without bias term."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Load the test data and compute accuracy\n",
    "Compute accuracy for test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9447900466562986"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_set = pd.read_csv('test.csv')\n",
    "\n",
    "ham_or_spam = test_set['label']\n",
    "test_set_target = []\n",
    "for i in range(len(ham_or_spam)):\n",
    "    if ham_or_spam[i] == \"ham\":\n",
    "        test_set_target.append(1)\n",
    "    else:\n",
    "        test_set_target.append(0)\n",
    "y_test = test_set_target\n",
    "test_set_target = np.transpose(np.matrix(test_set_target))\n",
    "\n",
    "test_set_content = test_set['sms'].as_matrix()\n",
    "test_set_corpus_tokenized = []\n",
    "for i in test_set_content:\n",
    "    test_set_corpus_tokenized.append(' '.join(tokenization(i)))\n",
    "\n",
    "tfidf_test = transformer.transform(vectorizer.transform(test_set_corpus_tokenized))\n",
    "X_test = tfidf_test.toarray()\n",
    "\n",
    "accuracy_test = compute_accuracy(X_test, test_set_target, w)\n",
    "\n",
    "accuracy_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Add bias term"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[ 1.83550768e+00],\n",
       "        [-2.66734942e-04],\n",
       "        [-2.33901988e-04],\n",
       "        ...,\n",
       "        [ 3.21408940e-04],\n",
       "        [ 2.42217571e-04],\n",
       "        [ 1.39297406e-04]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = add_bias_term(X)\n",
    "# initialize w to be a matrix of dimension 3000 x 1 with all elements to be 0\n",
    "w = np.transpose(np.matrix(np.zeros(np.shape(X)[1])))\n",
    "w = gradient_descent(X, training_set_target, w, 1, 0.05)\n",
    "\n",
    "w"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute accuracy for training set with bias term"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.861"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy = compute_accuracy(X, training_set_target, w)\n",
    "\n",
    "accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute accuracy for test set with bias term"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8716951788491446"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test = add_bias_term(X_test)\n",
    "accuracy_test = compute_accuracy(X_test, test_set_target, w)\n",
    "\n",
    "accuracy_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Cross validation\n",
    "In practice, 10-fold cross validation works extremely slowly. Also, find out that in both models (with or without bias term), accuracies do not change obviously when changing hyperparameter within a small range (around 0 - 0.1). However, while the hyperparameter exceeds this range, the iteration of gradient descent pauses early in order to avoid 'overshooting'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# return the lmd that maximize the accuracy\n",
    "def cross_validation(X, w, y, eta):\n",
    "    # divide the dataset into 10 folds\n",
    "    kf = KFold(n_splits = 10)\n",
    "    training_cost_avg = 0\n",
    "    test_cost_avg = 0\n",
    "    accuracy_history = []\n",
    "    for train_index, test_index in kf.split(X):\n",
    "        X_train, X_test = X[train_index], X[test_index]\n",
    "        y_train, y_test = y[train_index], y[test_index]\n",
    "    lmd = np.linspace(0, 1, num=100)\n",
    "    for i in range(len(lmd)):\n",
    "        w = np.transpose(np.matrix(np.zeros(np.shape(X_train)[1])))\n",
    "        w = gradient_descent(X_train, y_train, w, eta, lmd[i])\n",
    "        accuracy = compute_accuracy(X_test, y_test, w)\n",
    "        accuracy_history.append(accuracy)\n",
    "    # find the index of this best lmd\n",
    "    best_lmd_index = np.argmax(accuracy_history)\n",
    "    # find it in the lmd array\n",
    "    lmd_best = lmd[best_lmd_index]\n",
    "    return lmd_best"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
